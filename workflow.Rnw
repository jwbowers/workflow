\documentclass[12pt]{article}
\usepackage{fancyvrb,natbib,url,array,dcolumn,float,booktabs,listings}
\usepackage[colorlinks=TRUE]{hyperref}

\title{How to improve your relationship with your future self.}

\author{Jake Bowers\thanks{I owe many thanks to Mark Fredrickson,
    Brian Gaines, Mark Fredrickson, Kieran Healy, Kevin Quinn and Cara
    Wong for direct help on this document and to Mika LaVaque-Manty
    and Ben Hansen for many useful discussions on this topic. The
    source code for this document may be freely downloaded and
    modified from \url{https://github.com/jwbowers/workflow}.}}

\date{\today}

\newcolumntype{.}{D{.}{.}{1.2}}

\usepackage[noae,nogin]{Sweave} %Declaring this explicitly so that I can modify the Sweave environment here rather in Sweave.sty which will be c
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=.5em,fontsize=\small}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=.5em,fontsize=\small}
\DefineVerbatimEnvironment{Scode}{Verbatim}  {xleftmargin=.5em,fontsize=\small}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
\SweaveOpts{keep.source=TRUE}  %Show comments
\fvset{fontsize=\small} %slightly smaller verbatim font

%% These next lines tell latex that it is ok to have a single graphic
%% taking up most of a page, and they also decrease the space arou
%% figures and tables.
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\setlength{\intextsep}{2ex}
\setlength{\floatsep}{2ex}
\setlength{\textfloatsep}{2ex}


\begin{document}
\maketitle


<<initialize,echo=FALSE,results=hide>>=
  ##First, just setup the \R environment:
  options(SweaveHooks=list(
            fig=function(){
              par(mar=c(3.5, 3, 1.1, 0),
                  pty="s",
                  mgp=c(1.5,0.5,0),
                  oma=c(0,0,0,0))},#,
            echo=function(){options(continue=" ") ##Don't show "+" prompts,
                            options(prompt=" ")}
            ),
          digits=3,
          scipen=8,
          width=100,
          error=function(){options(prompt="> ",continue="+ ");NULL}
          )

@


\begin{verse}
Do I contradict myself? \\
Very well then I contradict myself, \\
(I am large, I contain multitudes.) \\
\citep{whitman1855song}
\end{verse}

An idea is born in a coffee shop, a seminar, a quiet walk. On this
gray day in 2011, the idea dispells February's doldrums. The student
rushes home, mind racing, the cold ignored.
% Now he knows why he went to graduate school.

This idea inspires a seminar paper in the spring. A conference
paper arises from the seminar paper in collaboration with another
student in 2012. A dissertation chapter descends from the conference
paper in 2013. Other dissertation chapters take up 2014. A submission
to a journal with the original co-author and a new collaborator
happens in 2015. Revision and resubmission wait until 2017 while
harried editors, reviewers and authors strive to balance teaching,
service, and life. By now, the three lucky collaborators work as
professors in three different universities. In 2018 a child is born
and a paper is published. The United Nations takes an interest in the
paper in 2019 and hosts a conference to discuss implications of the
research. In 2020 a first year graduate student in a coffee shop has
an idea that challenges the results in the now famous paper. What
would happen if the authors had controlled for X? Or run one more MCMC chain?
Or chosen a different likelihood function?  Will the United Nations
(now eager to act based the paper) make a wrong move?

The first author convenes a three way video conference with the other
collaborators during his homeward commute after putting his flying car
in auto-drive mode.\footnote{One assumes that video chatting during
  manual driving of flying cars will have been outlawed in his state
  in 2017.} The group must go back to the analyses. Which ones?  The
ones from 2011?  Or 2012? Or 2018? Where are the files? The next day,
one member of the group who has kept some hard-drives around out of
nostalgia finds some of the files.\footnote{This is the same guy who
  still owns cassette tapes and compact discs.} Now the checks and
re-analyses should be easy. Right? The student, now professor, should
remember the reason for those bits of code (or at least should
remember which series of mouse clicks were used to produce the numbers
for that crucial table --- and that mousing should not be time
consuming to redo exactly as they were done in 2011, or was it
2015?). Right? And, of course, the way Microsoft
Word/Stata/SPSS/R/LISREL stores files and the way that current
machines in 2020 read and write them is the same --- since Windows and
Mac OS X have always existed and will always continue to exist more or
less as they currently exist? Right? And the group knows exactly which
bit of code produced which table and which figure, right? And they
wrote their code following Nagler's Maxims \citep{nagler1995coding}
and King's Replication Standard \citep{king1995replication} right?

Right? Wrong? If the collaborators find themselves answering ``wrong''
to many of the questions posed here then reproducing the analyses will
take a lot of time if such reproduction is possible at all. This essay
provides some suggestions for practices which will make such
reproduction occur much more easily and quickly in the event that
famous papers require special scrutiny. Specifically, this piece aims
to amplify some of what we already ought to know from
\citet{nagler1995coding} and \citet{king1995replication}, and to
update some of those ideas given current practices, platforms, and
propensities.

%%% This data frame will be used throughout the analyses.
%%% Putting it up top, so we can use it in the comments section.
<<read.data,echo=FALSE,results=hide>>=
##  This read.csv line is slightly more complex than needed because of
##  the particular encoding used for characters in the source file.
dem.nations.df<-read.csv(url("http://www.hks.harvard.edu/fs/pnorris/Data/Democracy%20CrossNational%20Data/Democracy%20Crossnational%20Data%20Spring%202009.csv",encoding="ISO_8859-1"),as.is=TRUE)
row.names(dem.nations.df)<-dem.nations.df$Natabrv ##make the row names more helpful
good.df<-na.omit(dem.nations.df[,c("Gini2004","protac2000","Nation","meanpr")])
@

\section{Data analysis is computer programming.}

All results (numbers, comparisons, tables, figures) should arise from
code, not series of mouse clicks or copying and pasting. If I wanted
to re-create the figure you created but including a new variable or
specification, I should be able to do so with just a few edits to the
code rather than knowledge of how you used your pointing device in
your graphical user interface.

Using R, for example, I might specify that the file \Verb+fig1.pdf+
was produced by the following commands in a file called
\Verb+fig1.R+.\footnote{The command \Verb+please-plot+ and some other
  R functions used in this essay come from the
  \Verb+MayIPleaseDoStatistics+ package which emphasizes politeness in
  data analysis. The short hand commands like \Verb+please-plot+ can
  be blocked and \Verb+may-I-please-have-a-plot+ can be required using
  \Verb+options(politeness=99)+ }

\begin{Verbatim}[fontsize=\small]
thedata<-read.csv("Data/thedata.csv")
pdf('fig1.pdf')
please-plot(outcome by explanatory using thedata. red lines please.)
dev.off()
\end{Verbatim}


Now, in the future if I wonder how ``that plot on page 10'' was
created: (1) ``that plot'' is from a file called ``fig1.pdf'' and
fig1.pdf was created in \Verb+fig1.R+. In a future where R still
exists, if I wanted to change the figure, I could edit the commands
quickly and easily to do so (since R commands are written in plain
text, and plain text is a format that will be around for a long time
to come). 

\paragraph{Principle} Know the provenance of your results so that your
future self or current collaborators can quickly and easily reproduce
your work (and make changes).% --- to know what
% commands created which figures and tables.

\section{No data analyst is an island.}
Data analysis involves a long series of decisions. Each decision
requires justification.  The body of the paper or book or report will
justify the most important decisions. Some decisions will be too small
and technical for inclusion in the paper itself. These need to be
documented in the code itself \citep{nagler1995coding}.

Comments --- unexecuted text inside of a script --- are a message to
your future self and other consumers of your work.  Read
. Comment your code. Luckily, if you are using
a literate programming practice (Sweave, odfWeave, using R2HTML,
etc..), you can write paragraphs to surround your code as well a
technical comments in the code itself. If you are not using a strictly
literate programming practice then use whatever commenting protocol
exists in your analysis language. Here are some examples:
\begin{verbatim}
# This is a comment in R.
# With a a second line.

* This is a comment in a Stata .do file
* You can also embed comments at the ends of lines using /* ... */

/* SAS uses a similar convention to Stata
   and allows multi-line comments like this.
*/

% Finally, the percent symbol is a comment in TeX.
\end{verbatim}



Using R, for example, the comment is marked using the
pound sign. Notice that I've left in a regression not run for the
paper directly, but run as a part of the due diligence process of
writing the paper.
 
<<reg1,echo=TRUE,results=verb>>=

 ##  Repeat the regression run for Table~\ref{tab:protest}
 ##  Do political rights predict protest reporting?
 lm1<-lm(protac2000 ~ I(Gini2004/100) + meanpr, data=good.df)
 ##  Q: Was it worthwhile controlling for political rights?
 lm2<-lm(protac2000~I(Gini2004/100),data=good.df)
 coef(lm2)[2]-coef(lm1)[2]
 ##  A: Yes (around 3/4 an act of protest difference)
@

If you need to track down something more than a small detail, but yet
the sleuthing is really a footnote to the article, then you can create
another file to explore and report on those questions for the research
group. The writing in such a memo can be more informal, but the norms
of reproducibility ought to be the same as for the main paper. After
all, you will be releasing both documents (probably) publicly upon
publication of the article if not before.

Notice one another benefit of coding for an audience: we learn by
teaching. By assuming that others will look at your code, you will be
more likely to write clearer code, or perhaps even to learn more about
what you are doing as you do it.

Comment liberally. Comments are discarded when R runs analysis or LaTeX turns
your poignant writings into a PDF, so only those who dig into your work will
see them. They are footnotes on the process of producing your research.

\section{Data analysis is commnication.}

The source code of any pdf document exchanged by the group must be
available and executable. Say you run some command and then make a
table or a graph. Then you produce a nice little report on your work
for use among the working group. Eventually we want to use pieces of
that report (tables, graphs, paragraphs) in a publishable paper. Thus,
that report must not have been created using copying and pasting. It
must be created using LaTeX (or HTML or OpenOffice or something else
that the working group standardizes on). And, in fact, it must be
created using the reproducible markup system like Sweave (if you are
using LaTeX and R as your working group tools)
\citep{Leis:2005}. Alternatively, you can put comments into your
report detailing where the figures and tables came from (i.e. which
command file produced them). Or include details about which code
produced which figures in your MANIFEST.txt.

In order to know that ``that plot on page 10'' is from a file called
``fig1.pdf'' you need to document it somehow. I can imagine a system
with very disciplined use of a word processor (like Open Office or
Word) --- perhaps using special auxiliary ``listoffigures.txt'' files
or something. In my experience, it is much easier just to use LaTeX
(which can be used in a very user friendly way via LyX).

The Sweave system is even nicer because you can have a LaTeX file with
R code right inside of it! I showed Sweave with plotting in
Figure~\ref{fig:giniprot}. Those who view the code for this essay will
see how Table~\ref{tab:protest} was also generated directly from a
regression object.

<<maketable, echo=FALSE, results=hide>>=
  lm1<-lm(protac2000~I(Gini2004/100)+meanpr,data=good.df)  #  Run the regression

##  The next lines extract relevant and useful output from the
##  regression object, following Neal Beck's TPM article 2010.
themat<-cbind(summary(lm1)$coef[,c("Estimate","Std. Error")],confint(lm1))
n<-length(lm1$fitted.values)
r2<-signif(summary(lm1)$r.squared,2)
est.sigma<-signif(summary(lm1)$sigma,2)
colnames(themat)[3:4]<-c("~{}","~{}")
row.names(themat)<-c("Intercept","Income Inequality (lower=more equal)","Mean Political Rights (lower=more rights)")

library(xtable)
thextab<-xtable(themat,align=c("l",rep(".",4)),
                caption="People living in countries with unequal
  income distributions report less protest activity to World Values
  Survey interviewers than people living in countries with relatively
  more equal income distributions, adjusting for average political
  rights as measured by Freedom House 1980--2000. Data from \\citep{norris2009data}.",label="tab:protest")
@


<<printtable,echo=FALSE,results=tex>>=
  print(thextab,include.colnames=FALSE,
        hline.after=NULL,
        table.placement="H",
        add.to.row=list(
          pos=list(0,3),
          command=c(
            "\\toprule \n &
             \\multicolumn{1}{c}{Coef} &
             \\multicolumn{1}{c}{Std. Err.} &
             \\multicolumn{2}{c}{95\\% CI} \\\\ \\cmidrule(r){2-5} ",
            paste("& \\multicolumn{4}{c}{n: ",n,",",
                  "   resid.sd:",signif(est.sigma),
                  ", R$^2$:",r2,"} \\\\ \\bottomrule",sep="")
            )
          ))
@


Right now it is hard to beat Sweave as a system for enhancing the
collaboration of research groups doing quantitative work in political
science. Something other than than Sweave will make our lives easier
in the future. Then we'll change.


\section{Meaningful code requires data.}

All files containing commands operating on data must refer to a data
file. A reference to a data file is a line of code the analysis
program will use to operate on (``load''/ ``open'' / ``get'' /
                                ``use'') the data file. One should not have to edit this line on
different computers or platforms in order to execute this command.
Using R, for example, all analysis files should have
\Verb+load('thedata.rda')+ or
\Verb+read.csv('http://www.mywebsite.org/Data/thedata.csv')+ or some
equivalent line in them, and `thedata.csv' should be stored in some
place easy to find (like in the same directory as the file or perhaps
                    in \Verb+'Data/thedata.rda'+).
It never hurts to drop in a comment about where the data is, if in doubt.

Where should one store data files? A very obvious solution is
always to make sure that the data file used by a command file is in the
same directory as the command file. More elegant solutions (pointed to
                                                            below) require all co-authors to have the same directory structure so
that \Verb+load('Data/thedata.rda')+ means the same thing on all computers
used to work on said project.

The principle of modularity \cite{nagler1995coding} suggests that you
separate data cleaning, processing, recoding, and merging from
analysis in different files. So, perhaps your analysis oriented files
will \Verb+load('cleandata.rda')+ or something but with a comment in the code
telling the future you (among others) that cleandata.rda was created
from create-cleandata.R which in turn begins with
\Verb+read.csv('dirtydata.csv')+ which, from a comment in the file, was
downloaded from
\Verb+http://www.greatfreedata.gov/dirtydata.csv+ on April 1,
2011. Such a data processing file will typically end with something
like \Verb+save('cleandata.rda')+ so that we are doubly certain about
the provenance of the data.

Now, if in 5 years we wonder where `cleandata.rda' came from, we might
\Verb+ grep cleandata *.R+ to search for occurrences of this file on
our Unix/OS X system. However, if such searching among files is a
burden, an even nicer solution is to maintain a file for each project
called ``MANIFEST.txt'' or ``INDEX.txt'' or ``README.txt'' which lists
the data and command files with brief descriptions of their functions
and relations.

The principle regarding data is to know where the data came from and
what operations were performed on which set of data. I have seen
command files which do not begin with a call to load data, and I have
despaired. What is the meaning of \Verb+please-fit(var1 with var2)+ if
I do not know where var1 var2 come from.



\section{Version control prevents clobbering and record history.}

Group work requires version control.\footnote{See X and Y in this
  issue for more discussion of what version control is.} Many people
are familiar with the ``Track Changes'' feature in modern WYSIWYG word
processors or the fact that Dropbox allows one to recover previous
versions of files. These are both kinds of version control. When
collaborating with yourself or others, it is useful to see what has
changed, to feel free to experiment and then to dump the experiment in
favor of previous work, to have multiple ``releases'' of the same
document (one to MPSA, one to APSR, one to your parents) without
requiring that spawn many copies of the same document and risk
confusion and clobbering. Clobbering is what happens when your future
self or your other collaborators saves an old version of a file over a
new version --- erasing good work by accident.

Of course if you rely on Dropbox or ``track changes'' for version
control, you must communicate with other folks in your group before
you edit existing files. Only one of you can edit and save a given
file at a time. This prevents your work (or your colleagues work) from
getting lost when you both try to save the same file on top of each
other. If you find that you need to work on the same files at
the same time, then you should work on establishing your own shared
version control system. Free options include launchpad, github,
sourceforge for open source projects (i.e. papers you are writing
which you are happy to share with others). Each of those services
include paid versions too. One may also use Dropbox as a kind of
server for version control: checking out files from the Dropbox
directory into a local working directory. (Notice that this is
different from directly working on files within your Dropbox managed
directories.)

We use subversion with our own research group, and I use it for all of
my own projects. Subversion and bazaar and git are all great. They
mainly differ in the extent to which you need to run a
server. Subversion requires a server and we are lucky that the
National Center for Supercomputing Applications provides such a server
for us. However, the price of such hosting may not be that much using
one of the many webhosting services out there. [See Y in this issue
for much more about version control and collaboration.]

Fancy version control systems are not required, however. I suspect
that Google Docs allows a kind of version tracking and collaboration
as well.  An excellent, simple, and robust version control system is
to merely rename your files with the date and time of saving them:
thedoc.tex becomes thedoc25-12-2011-23:50.tex.  Be sure to include
year in the file names --- remember, the life of an idea is measured
in years. Also, if you use this method, spend a little extra time to
ensure that you do not clobber files when you make typos in the
file-names. And, you will find yourself spending extra time
reconciling changes made by different collaborators by hand that
modern version control systems (particularly the distributed kind
these days) take care of quickly and easily.

If you are wise enough to have saved your documents as plain text
(Such as LaTeX source (with or without R/Stata code chunks)
 \footnote{A quick Google search of ``Sweave for Stata'' turned up lots
           of resources for literate programming with Stata.} then you can
 easily compare documents using the many utilities available for
 comparing text files [FileMerge on OS X is pretty, but ediff and diff
                       for Unix are very useful, I am sure that Windows has many other
                       options]. Adobe Acrobat allows one to compare differences in pdf
 files. OpenOffice supports a ``Compare Documents'' option.

 When you reach certain milestones you can rename the file accordingly:
 thedocAPSA2009.tex --- for the one sent to discussants at APSA --- or
 thedocAPSR2015.tex --- for the version eventually sent to the APSR six
 years after you presented it at APSA. The systems I mentioned above
 all allow this kind of thing and are much more elegant and capable,
 but you can do it by hand too as long as you don't mind taking up a
lot of disk space and having many many ``thedoc...''  files around.



 \section{Minimize error by testing.}

 Imagine that you reported bootstrap confidence intervals in your
 famous article. Now the statisticians at the UN are worrying about
 your bootstrap confidence interval. You'd like to evaluate your
 bootstrap procedure. Although nice code exists for bootstrapping
 linear models, no nice code exists to bootstrap the bootstrap. Of
 course, the code required is not complex, but since you are writing
 your own code you worry about getting it right. Now, 9 years after
 the idea, you've had lots of time to appreciate problems arising from
 bugs and errors in data analysis and code.

 Now, if you had a moment to think in between teaching that new class,
 reading books for the awards committee, reading application files for
 the admissions committee, staying home with a sick child, and
 undertaking the odd bit of your own current research, you might say
 to yourself, ``Before I write new code, I should write a test of the
 code. I should write a little bit of code that let's me know that my
 double-bootstrap procedure actually does what it is supposed to do.''

 Of course, this idea, like most others, is not new. When large groups
 of programmers write code for multi-million dollar programs the
 question about avoiding error looms large. The idea of
 \href{http://en.wikipedia.org/wiki/Test-driven_development}{test
   driven development} and the idea that one ought to create tests of
 \href{http://en.wikipedia.org/wiki/Unit_testing}{small parts of one's
   code} arose to address such concerns. For the social scientist
 collaborating with her future self and/or a small group of
 collaborators.

Here is an example of this idea in a very simple form. I want to write a function to multiply a number by 2. If my function works, when I give it the number 4, I should see it return the number 8 and when I give it -4, I should get -8.

<<unit.test>>=

  ##  The test function:
  test.times.2.fn<-function(){
    ##  This function tests times.2.fn
    if(times.2.fn(thenumber=4) == 8 &
       times.2.fn(thenumber=-4) == -8){
      print("It works!")
    }else{print("It does not work!")}
  }

##  The function:
times.2.fn<-function(thenumber){
  ##  This function multiplies a scalar number by 2
  ##  thenumber is a scalar number
  thenumber+2
}

##Use the test function
test.times.2.fn()
@

Ack! I mistyped ``+'' for ``*''. Good thing I wrote the test!

\section{File names send messages to your future self.}

Name your files with evocative and descriptive names. Your
collaborators are less likely to call you at midnight asking for help
if your files are named ``inequality-and-protest-analyses.R'' than if your files are
called ``temp9'' or ``supercalifragilisticexpialidocious.'' (Note the
                                                            use of the extension .R to tell us that the file contains R
                                                            commands. Use extensions like this as a standard practice to help you
                                                            and your computer get along.)



\section{Copy and improve on others' examples.}

Lots of people are thinking about ``reproducible research'' and
``literate programming'' these days. Google those terms. Of course the
devil is the details: Here I list a few of my own attempts at enabling
reproducible research. You'll find many other inspiring examples on
the web. Luckily, the open source ethos aligns nicely with academic
incentives, so we are beginning to find more and more people offering
their files online for copying and improvement. By the way, if you do
copy and improve, it is polite to alert the person from whom you made
the copy about your work.

I have experimented with three systems so far: (1) for one paper we
simply included a Sweave document and data files into a compressed
archive \cite{bowers2005dataverse}; (2) for another more computing
intensive paper we assembled a set of files which enabled reproduction
of our results using the ``make'' system \citep{bowers2008dataverse};
and (3) recently I have tried the ``compendium'' approach
\citep{gentleman2005reproducible,gentleman2007statistical} which
embeds an academic paper with the R package system
\cite{bowers2011dataverse}. The benefit of this last system is that
one is not required to have access to unix: the compendium is
downloadable from within R using \Verb+install.packages()+ and it
viewable using the \Verb+vignette()+ function. The idea that one ought
to be able to install and run and use an academic paper just as one
installs and uses statistical software packages is very attractive and
I anticipate that it will become ever easier to turn papers into R
packages as creative and energetic folks turn their attention to the
question of reproducible research.

\section{Remember that research ought to be credibile communication.}
\begin{quote}
  [I]f the empirical basis for an article or book cannot be
  reproduced, of what use to the discipline are its
  conclusions?  What purpose does an article like this serve?
  \cite[445]{king1995replication}
\end{quote}

We all always collaborate. Many of us collaborate with groups of
people at one moment in time as we race against a deadline. All of us
collaborate with ourselves over time.\footnote{What is a reasonable
  time-span for which to plan for self-collaboration on a single idea?
  Ask your advisers how long it took them from idea to dissertation to
  book (articles).}  The time-frames over which collaboration are
required --- whether among a group of people working together or
within a single scholar's productive life or probably both --- are
much longer than any given version of any given software will easily
exist. \href{http://en.wikipedia.org/wiki/Plain_text#Usage}{Plain
  text} is the exception. Thus, even as we extol version control
systems, one must have a way to ensure future access to them in a form
that will still be around when sentient cockroaches finally join
political science departments (by then dominated by cetaceans after
humans are mostly uploads).\footnote{The arrival of the six-legged
  social scientists revives Emacs and finally makes Ctrl-a Ctrl-x
  Esc-x Ctrl-c a \href{http://kieran.healy.usesthis.com/}{reasonable
    key combination}.}

But what if the UN never hears of your work, or, by some cruel fate, that your
article does not spawn debate? Why then would you spend time to
communicate with your future self and others? My own answer to this
question is that I want my work to be credible and useful to myself
and other scholars even if each article does not immediately change
the world.  What I report in my data analyses should have two main
characteristics: (1) the findings of the work should not be a matter
of opinion; and, (2) other people should be able to reproduce the findings. That is,
what the work represents is not a matter of opinion but is a shared
experience --- and an experience shared without respect to the
identities of others (although requiring some shared technical
training and research resources).
% Such work should make us change how
% we act --- or at least, it ought to stand on stronger epistemological
% ground than other claims about experiences which ought to be shared or
% shareable.

Assume we want others to believe us when we say something. More
narrowly, assume we want other people to believe us when we say
something about data: ``data'' here can be words, numbers, musical
notes, images, ideas, etc \ldots The point is that we are making some
claims about patterns in some collection of stuff. Now, it
might be easy to convince others that ``this collection of stuff
is different from this collection of stuff'' if those people were
looking over our shoulders the whole time that we made decisions about
collecting the stuff and broke it up into understandable
parts and reorganized and summarized it. Unfortunately, we can't assume that people are willing to
         shadow a researcher throughout her career. Rather, we do our work
         alone or in small groups and want to convince other distant and future people
         about our analyses.

         Now, say your collections of stuff are large or complex and your
         chosen tools of analyses are computer programs. How can we convince
         people that what we did with some data with some program is credible:
           not a matter of whim or opinion, and reproducible by others who didn't
shadow us as we wrote our papers? This essay has suggested a few
concrete ways to enhance the believeability of such scholarly work. In
addition, these actions (as summarized in the section headings of this
essay) make collaboration within research groups more effective.
Believeability comes in part from reproducibility and research groups
often need to be able to reproduce in part or in whole what different
people in the group have done.

In the end, following these practices and those recommended by X and Y
in this issue allows your computerized analyses of your collections
stuff of to be credible.  Finally, if the UN quibbles with your
analyses, your future self can shoot the archive required to reproduce
your work (in still intelligible plain text, analyzed using commented
code so that folks can translate to whatever system succeeds R, or
since you used R, you can include a copy of R and all of the R
packages you used in your final analyses in 2018 in the archive
itself.) You can say, ``Here is everything you need to reproduce my
work." To be extra helpful you can add ``Read the README file for
futher instructions." And then you can get on with your life: maybe
the next great idea will occur when your 4-year-old asks a wacky
question after stripping and painting her overly cooperative
1-year-old brother purple, or teaching a class, or in a coffee shop,
or on a quiet walk.

\bibliographystyle{apsr}
\bibliography{/Users/jwbowers/Documents/BIB/trunk/big}


\end{document}
<<>>=
options(prompt="> ",continue="+ ")
@
