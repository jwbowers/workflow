---
title: How to improve your relationship with your future self.
author:
- name: Jake Bowers
  affiliation: <jwbowers@illinois.edu>,  University of Illinois at Urbana-Champaign, Departments of Political Science and Statistics; Fellow, White House Social and Behavioral Sciences Team. Many thanks to Mark Fredrickson, Brian Gaines, Kieran Healy, Kevin Quinn and Cara Wong for direct help on this document and to Mika LaVaque-Manty and Ben Hansen for many useful discussions on this topic. The source code for this document may be freely downloaded and modified from <https://github.com/jwbowers/workflow>.
- name: Maarten Voors
  affiliation: <maarten.voors@wur.nl>, Wageningen University, Development Economics Group
bibliography: references.bib
csl: american-political-science-review.csl
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
    pdf_document:
      keep_tex: true
      number_sections: true
      fig_width: 7
      fig_height: 7
      fig_caption: true
      template: bowersarticletemplate.latex
      pandoc_args: [
      "-M", "secPrefix=\\S",
      "--filter", "/usr/local/bin/pandoc-crossref"
      ]
      md_extensions: +autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
...



```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.

## To make the html file do
## render("exploration4.Rmd",output_format=html_document(fig_retina=FALSE))
## To make the pdf file do
## render("exploration4.Rmd",output_format=pdf_document())
## to debug, try render("thefile.Rmd", clean=FALSE) so that you can see the intermediate files

require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='footnotesize',
  out.width='.9\\textwidth',
  message=FALSE,
  comment=NA)
```

```{r initialize,echo=FALSE}
##First, just setup the R environment for today:
if(!file.exists('figs')) dir.create('figs')

options(SweaveHooks=list(fig=function(){
			   par(mar=c(3.5, 3, 1.1, 0),
			       pty="s",
			       mgp=c(1.5,0.5,0),
			       oma=c(0,0,0,0))},
			 echo=function(){options(continue=" ") ##Don't show "+" prompts,
			 options(prompt=" ")
			 }),
	digits=4,
	scipen=8,
	width=132
	)

#options(error=function(){options(prompt="> ",continue="+ ");NULL})
```

> | Do I contradict myself?
| Very well then I contradict myself,
| (I am large, I contain multitudes.)
| [@whitman1855song]


> If you tell the truth, you don't have to remember anything. [@twain1975mark]

Memory is tricky. When we really want to remember something we should practice it until it becomes internalized. In fact, learning requires effort. When we do not practice, we tend to be overconfident in our recall abilities [@koriat2005illusions] and false memories can be implanted.^[See the following site for a nice overview of what we know about memory including the fact that learning requires practice: <http://www.spring.org.uk/2012/10/how-memory-works-10-things-most-people-get-wrong.php>. On false memory see Wikipedia and linked studies <https://en.wikipedia.org/wiki/False_memory>.] If we cannot count on memory, then how can we do science?

How long does it take from planning a study to publication and to the first reproduction of the study after publication? Is three years too short? Is ten years too long? We suspect that few of our colleagues in the social and behavioral sciences conceive of, field, write, and publish a study in faster than about three years. We also suspect that, if some other scholar decides to re-examine the analyses of a published study, that it will occur after publication, and that this new scholarly activity of learning from one anothers' data and analyses, can occur at anytime, years past the initial publication of the article.^[We note that sometimes this reproduction occurs within the context of classes and learning about statistics and data analysis. Othertimes, this process is organized more broadly (cite IIIE program, cite Open Science Foundation on psychology).]

If we cannot count on our memories about why we made such and such a design or analysis decision, then what should we do? How can we minimize regret with our past decisions? How can we improve our relationship with our future self?  This essay is a heavily revised version of @bowers2011tpm and provides some suggestions for practices that will make such reproduction occur much more easily and quickly in the event that famous papers require special scrutiny.  Specifically, this piece aims to amplify some of what we already ought to know,  and to add to some of those ideas
given current practices, platforms, and possibilities.^[@king1995replication and @nagler1995coding were two of the first pieces introducing these kinds of ideas to political scientists. Now, the efforts to encourage transparency and ease of learning from the data and analyses of others has become institutionalized with the DA-RT inititiave <http://www.dartstatement.org/> @lupia2014openness. These ideas are spreading beyond political science as well @freese2007replication and @asendorpf2013recommendations.] This essay is an attempt at providing practical advice about how to *do work* such that one complies with such recommendations as a matter of course and, in so doing, can focus personal regret on other bad past decisions but not on decisions about data analysis and the production of scholarly papers.

```{r read.data,echo=FALSE,results='hide', cache=TRUE}
## This data frame will be used throughout the analyses.
## Putting it up top, so we can use it in the comments section.
library(readstata13)
dem.nations.df<-read.dta13("https://www.dropbox.com/s/huhqag5rudwtbno/Democracy%20Cross-National%20Data%20V4.1%2009092015.dta?dl=1&pv=1&raw=1",convert.factors=FALSE)

row.names(dem.nations.df)<-dem.nations.df$Nation ##make the row names more helpful
good.df<-na.omit(dem.nations.df[,c("Gini2004","WVS5_protest","Nation","meanpr")])
good.df$gini04<-good.df$Gini2004/100
good.df$protest05<-good.df$WVS5_protest
```

We organize the paper around a series of homilies that lead to certain concrete actions.

# Data analysis is computer programming. {#sec:data-analys-comp}

All results (numbers, comparisons, tables, figures) should arise from
code, not from a series of mouse clicks or copying and pasting. If I wanted
to re-create the figure from 2011 but include a new variable or
specification, I should be able to do so with just a few edits to the
code rather than knowledge of how I (or you) used a pointing device in
your graphical user interface some years ago.

Using R [@R:2011], for example, I might specify that the file `fig1.pdf` was
produced by the following commands in a file called `fig1.R`.^[The command
`please-plot`and some other R functions used in this essay come from the
`MayIPleaseDoStatistics` package which emphasizes politeness in data analysis.
Functions like `please-plot`can be blocked and more polite versions such as
`may-I-please-have-a-plot`can be required using `options(politeness=99)`. We do
not recommend setting negative values for the politeness option should you use this package.]

```{r eval=FALSE}
## This file produces a plot relating the explanatory variable to the outcome.
thedata <- read.csv("Data/thedata-15-03-2011.csv") ## Read the data
pdf("fig1.pdf") ## begin writing to the pdf file
please-plot(outcome by explanatory using thedata. red lines please.)
please-add-a-line(using model1)
## Note to self: a quadratic term does not add to the substance
## model2 <- please-fit(outcome by explanatory+explanatory^2 using thedata
## summary(abs(fitted(model1)-fitted(model2)))
dev.off() ## stop writing to the pdf file
```


Now, in the future if I wonder how 'that plot on page 10' was
created, I will know: (1) 'that plot' is from a file called
`fig1.pdf`and (2) `fig1.pdf`was created in
`fig1.R`. In a future where R still exists, changing the figure
will require quick edits of commands already written.  In a future
where R does not exist, I will at least be able to read the plain text
R commands and use them to write code in my new favorite statistical
computing language: R scripts are written in
[plain text](http://en.wikipedia.org/wiki/Plain_text), and plain
text is a format that will be around as long as computer programmers
write computer programs.^[Since R is open
  source, I will also be able to download an old version of R, download an
  old-fashioned open-source operating system (like Ubuntu 10), and
  run the old-fashioned statistical computing environment in the
  old-fashioned operating system in a virtual machine on my
  new-fashioned actual machine.]

Moreover, realize that *file names send messages* to your future
self. This means that if you name your files with evocative and descriptive names, your
collaborators are less likely to call you at midnight asking for help and you will remove some regret from your future self. For example, if you are studying inequality and protest, you might try naming a file something like  `inequality-and-protest-figures.R` instead of  `temp9`or
`supercalifragilisticexpialidocious`.  By the way, the extension
`.R` tells us and the operating system that the file contains R
commands. This part of the filename enables us to quickly search our
antique hard drives for files containing R scripts.

**Step 1** If we know the provenance of results, future or current collaborators can quickly and easily reproduce
and thus change and improve upon the work.

# No data analyst is an island for long.

Data analysis involves a long series of decisions. Each decision
requires justification.  Some decisions will be too small and
technical for inclusion in the published article itself. These need to
be documented in the code itself [@nagler1995coding]. Paragraphs
and citations in the publication will justify the most important
decisions. So, one must code to communicate with yourself and
others. There are two main ways to avoid forgetting the reasons you
did something with data: comment your code and tightly link your code
with your writing.^[One can also try the R command
  `put-it-in-my-mind(reason,importance="high")` to firmly place a
  reason for a decision into the mind of the analyst. I myself have
  not had much luck with this function.]

## Code is communication: Comment code

Comments, unexecuted text inside of a script, are a message to
collaborators (including your future self) and other consumers of your
work. In the above code chunk, I used comments to explain the lines to
readers unfamiliar with R and to remember that I had tried a different
specification but decided not to use it because adding the squared
term did not really change the substantive story arising from the
model.^[R considers text marked with `#` as a comment.]
Messages left for your future self (or near-future others) help
retrace and justify your decisions as the work moves from seminar
paper to conference paper to poster back to paper to dissertation and
onwards.

Notice one other benefit of coding for an audience: we learn by
teaching. By assuming that others will look at your code, you will be
more likely to write clearer code, or perhaps even to think more
deeply about what you are doing as you do it.

Comment liberally. Comments are discarded when R runs analysis or LaTeX or pandoc
turns plain text into pdf or html files so only those who dig into
your work will see them.

## Code to communicate: Literate programming.

> Let us change our traditional attitude to the construction of
  programs: Instead of imagining that our main task is to instruct a
  computer what to do, let us concentrate rather on explaining to
  human beings what we want a computer to do.
  [@knuth1984literate, p.97]

Imagine you discover something new (or confirm something old). You
produce a nice little report on your work for use in discussions of
your working group or as a memo for a web or reviewer appendix. The
report itself is a pdf file or some other format which displays page
images to ease reading rather than to encourage reanalysis and
rewriting. Eventually pieces of that report (tables, graphs,
paragraphs) ought to show up in, or at least inform, the publishable
paper. Re-creating those analyses by pointing, clicking, copying, or
pasting would invite typing error and waste time. Re-creating your
arguments justifying your analysis decisions would also waste
time.

More importantly, we and others want to know why we did what we
did. Such explanations may not be very clear if we have some pages of
printed code in one hand and a manuscript in the other. Keep in mind
the distinction between the 'source code' of a document (i.e. what
computation was required to produce it) and the visible, type-set page
image. Page images are great for reading, but not great for
reproducing or collaborating. The source code of any document
exchanged by the group must be available and executable.^[As of the 2016 version of this paper, this idea is now widespread and made much easier than before via online services for code sharing and collaboration such as [GitHub](http://github.com) and [BitBucket](http://bitbucket.org).]

How might one avoid these problems?
[Literate programming](http://en.wikipedia.org/wiki/Literate_programming) is the practice of weaving code into a document ---
paragraphs, equations, and diagrams can explain the code, and the code
can produce numbers, figures, and tables (and diagrams and even equations and
paragraphs). Literate programming is not merely fancy commenting but
is about enabling the practice of programming itself to facilitate easy
reproduction and communication.

For example, in [@sec:data-analys-comp], I suggested that we know
where 'that plot on page 10 'comes from by making sure we had a
`fig1.pdf` file produced from a clearly commented plain text file
called something like `fig1.R`.  An even easier solution would be to
directly include a chunk of code to produce the figure inside of the
paper itself. This paper, for example, was written in plain text using
markdown markup with R code chunks to make things like
Figure \ref{fig:giniprot}. This combination of markdown and R is called
[R Markdown](http://rmarkdown.rstudio.org).

```{r fig1plotcode,eval=FALSE,echo=TRUE}
##  Make a scatterplot of Protest by Inequality
par(bty="n",xpd=TRUE,pty="s",tcl=-.25)
with(good.df,plot(gini04,protest05,
                  xlab='Gini Coefficient 2004 (UNDP)',
                  ylab='Proportion Reporting Protest Activities\n(World Values Survey 2005)',
                  cex=.8))
##  Label a few interesting points
with(good.df[c("Brazil","United Kingdom","United States","Sweden","Chile"),],
     text(gini04,protest05,labels=Nation,srt=0,cex=.6,pos=3,offset=.1))
```

```{r giniprot,message=FALSE,echo=FALSE,fig=TRUE,include=TRUE,width=4,height=4,fig.cap="Average number of Protest activity by income inequality.",out.width=".5\\textwidth",fig.env="figure",fig.lp="fig:",fig.pos="H",fig.align="center"}
##  Make a scatterplot of Protest by Inequality
par(bty="n",xpd=TRUE,pty="s",tcl=-.25)
with(good.df,plot(gini04,protest05,
                  xlab='Gini Coefficient 2004 (UNDP)',
                  ylab='Proportion Reporting Protest Activities\n(World Values Survey 2005)',
                  cex=.8))
##  Label a few interesting points
with(good.df[c("Brazil","United Kingdom","United States","Sweden","Chile"),],
     text(gini04,protest05,labels=Nation,srt=0,cex=.6,pos=3,offset=.1))
```


Markdown (and LaTeX and HTML) all have ways to cross-reference within a
document. For example, by using `\label{fig:giniprot}` in LaTeX or
`{#fig:giniprot}` in the pandoc version of markdown, I do not need to keep
track of the figure number, nor do extra work when I reorganize the document in
response to reviewer suggestions. Nor do I need a separate `fig1.R`file or
`fig1.pdf` file.  Tables and other numerical results are also possible to
generate within the source code of a scholarly paper. Those who view the code
for this essay will see how Table \ref{tab:protest} was also generated directly
from a regression object.\footnote{@beck2010reg inspired this particular
presentation of a linear model.}

```{r maketable, echo=FALSE, results='hide'}
  lm1<-lm(protest05~gini04+meanpr,data=good.df)  #  Run the regression

##  The next lines extract relevant and useful output from the
##  regression object, following Neal Beck's TPM article 2010.
themat<-cbind(summary(lm1)$coef[,c("Estimate","Std. Error")],confint(lm1))
n<-length(lm1$fitted.values)
r2<-signif(summary(lm1)$r.squared,2)
est.sigma<-signif(summary(lm1)$sigma,2)
colnames(themat)[3:4]<-c("~{}","~{}")
row.names(themat)<-c("Intercept","Income Inequality (lower=more equal)","Mean Political Rights (lower=more rights)")

library(xtable)
thextab<-xtable(themat,align=c("l",rep(".",4)),digits=1,
                caption="People living in countries with unequal
  income distributions report less protest activity to World Values
  Survey interviewers than people living in countries with relatively
  more equal income distributions, adjusting for average political
  rights as measured by Freedom House 1980--2000. Data from [@norris2015data].",label="tab:protest")
```


```{r printtable,echo=FALSE,results='asis',message=FALSE}
  print(thextab,include.colnames=FALSE,
        hline.after=NULL,
        table.placement="H",
	comment=FALSE,
        add.to.row=list(
          pos=list(0,3),
          command=c(
            "\\toprule \n &
             \\multicolumn{1}{c}{Coef} &
             \\multicolumn{1}{c}{Std. Err.} &
             \\multicolumn{2}{c}{95\\% CI} \\\\ \\cmidrule(r){2-5} ",
            paste("& \\multicolumn{4}{c}{n: ",n,",",
                  "   resid.sd: ",signif(est.sigma),
                  ", R$^2$: ",r2,"} \\\\ \\bottomrule",sep="")
            )
          ))
```




Literate data analysis is not the same as Sweave, even if Sweave is a
nice implementation.^[The R project has a task view devoted to
  [reproducible research](http://cran.r-project.org/web/views/ReproducibleResearch.html) listing many of the different approaches to literate
  programming for R.}  If your workflow does not involve R, you can
still implement some of the principles here. Imagine creating a style
in Microsoft Word called 'code 'which hides your code when you print
your document, but which allows you to at least run each code chunk
piece by piece (or perhaps there are ways to extract all text of style
code from a Microsoft Word document). Or imagine just using some
other kind of indication linking paragraphs to specific places in code
files. There are many ways that creative people can program in a
literate way.

Literate programming need not go against the principle of modular data
analysis [@nagler1995coding]. In my own work I routinely have
several different Sweave files that fulfill different functions, some
of them create \LaTeX code that I can `\input` into my
`main.tex` file, others setup the data, run simulations, or allow
me to record my journeys down blind alleys. Of course, when we have
flying cars running on autopilot, perhaps something other than
Sweave will make our lives even easier. Then we'll change.

**Step 2**
We analyze data in order to explain something about the world to other
scholars and policy makers. If we focus on explaining how we got our
computers to do data analysis to human beings, we will do a better job
with the data analysis itself: we will learn as we focus on teaching,
and we will avoid errors and save time as we ensure that others
(including our future selves) can retrace our steps. A document than
can be 'run' to reproduce all of the analyses also instills
confidence in readers and can more effectively spur discussion and
learning and cumulation of research.

# Meaningful code requires data.

All files containing commands operating on data must refer to a data
file. A reference to a data file is a line of code the analysis
program will use to operate on ('load'/ 'open '/ 'get '/
'use') the data file. One should not have to edit this line on
different computers or platforms in order to execute this command.
Using R, for example, all analysis files should have
`load("thedata.rda")`or `read.csv("thedata.csv")`or some
equivalent line in them, and `thedata.csv`should be stored in
some place easy to find (like in the same directory as the file or
perhaps in `"Data/thedata.rda"`). Of course, it never hurts to
drop in a comment pointing to the data file.

[MAARTEN IDEAS HERE? PLUS ANOTHER SECTION ABOUT ORGANIZATION?]

Where should one store data files? An obvious solution is always to
make sure that the data file used by a command file is in the same
directory as the command file. More elegant solutions require all
co-authors to have the same directory structure so that
`load("Data/thedata.rda")` means the same thing on all computers
used to work on the project. This kind of solution is one of the
things that formal version control systems do well (as discussed
a bit more in [@sec:vers-contr-prev]).

The principle of modularity suggests that you separate data cleaning,
processing, recoding, and merging from analysis in different files
[@nagler1995coding]. So, perhaps your analysis oriented files will
`load("cleandata.rda")`and a comment in the code will alert the future you
(among others) that `cleandata.rda`was created from `create-cleandata.R` which
in turn begins with
`read.csv(\url("http://www.greatfreedata.gov/dirtydata.csv"))`. Such a data
processing file will typically end with something like
`save("cleandata.rda")`so that we are doubly certain about the provenance of
the data.\footnote{Of course, if you need math or paragraphs to explain what is
happening in these files, you might prefer to make them into R markdown or
Sweave files, for which the conventional extension is .Rmd or .Rnw
respectively. So you'd have `create-cleandata.Rnw` written as a mixture of
laTeX and R which might explain and explore the different coding decisions you
made, perhaps involving a factor analysis and diagnostic plots.}

Now, if in the future we wonder where `cleandata.rda`came from, we
might search for occurrences of `cleandata` in the files on our
system. However, if such searching among files is a burden, an even
nicer solution is to maintain a file for each project called
`MANIFEST.txt` or INDEX.txt or README.txt which lists the
data and command files with brief descriptions of their functions and
relations.

**Step 3** We should know where the data came
from and what operations were performed on which set of data.

In the good old days, when we executed our LISREL code in batch mode,
we had no choice but to tell the machine clearly, in a few easy to
understand and informative lines, what files (with filenames no longer
than 8 characters) to use. See for example, this extremely clear code snippet:

```
DA NI=19 NO=199 MA=CM
LA=basic.lab
CM FI=basic.cov
```

The fact that I need to articulate this idea at all arises because of
graphical user interfaces: it is all too easy to use the mouse to load
a data file into memory and then to write a script to analyze this
file without ever noting the actual name or location of the data file.

# Version control prevents clobbering, reconciles history, and helps organize work. {#sec:vers-contr-prev}

Group work requires version control.^[@fredrickson2011tpm and
@healy2011tpm in this issue also explain what version control is and why we
might want to use it.] Many people are familiar with the track changes
feature in modern WYSIWYG word processors or the fact that Dropbox allows one
to recover previous versions of files. These are both kinds of version control.
More generally, when we collaborate, we'd like to do a variety of actions with
our shared files. Collaboration on data analytic projects is more productive
and better when (1) it is easy to see what has changed between versions of
files; (2) members of the team feel free to experiment and then to dump parts
of the experiment in favor of previous work while merging the successful parts
of the experiment into the main body of the paper; (3) the team can produce
have 'releases' of the same document (one to MPSA, one to APSR, one to your
parents) without spawning many possibly conflicting copies of the same
document; (4) people can work on the same files at the same time without
conflicting with one another (and can reconcile their changes without too much
confusion and clobbering).  Clobbering is what happens when your future self or
your current collaborator saves an old version of a file over a new version,
erasing good work by accident.

Of course if you rely on Dropbox or 'track changes' in Word for version
control, you must communicate with other folks in your group before you edit
existing files. Only one of you can edit and save a given file at a time. This
prevents your work (or your colleagues work) from getting lost when you both
try to save the same file on top of each other. If you find that you need to
work on the same files at the same time, then you should work on establishing
your own shared version control system. Free options include launchpad, github,
sourceforge for open source projects (i.e. papers you are writing which you are
happy to share with others as you write). As of this writing, Google Docs does
an excellent job with shared editing of files as long as your are online. Each
of those services include paid versions too. One may also use Dropbox as a kind
of server for version control: for example, one may copy files from the Dropbox
directory into a local working directory so as to avoid clobbering and then
work on merging changes by hand before copying back to the Dropbox directory
and replacing existing files. MORE PERHAPS ON GOOGLE? LESS ON DROPBOX?

We use git with our own research group, and I use it for all of
my own projects. MORE ON GITHUB

Of course, one may take advantage of many of the benefits of formal
version control systems with some disciplined systems for file and
directory organization.  An excellent, simple, and robust version
control system is to rename your files with the date and time of
saving them: thedoc.tex becomes thedoc25-12-2011-23:50.tex.  Be sure
to include year in the file names --- remember, the life of an idea is
measured in years. If you are wise enough to have saved your documents
as plain text then you can easily compare documents using the many
utilities available for comparing text files.^[Adobe Acrobat
  allows one to compare differences in pdf files. OpenOffice supports
  a 'Compare Documents 'option. And Google Docs will report on the
  version history of a document.] When you reach certain milestones
you can rename the file accordingly: thedocAPSA2009.tex --- for the
one sent to discussants at APSA --- or thedocAPSR2015.tex --- for the
version eventually sent to the APSR six years after you presented it
at APSA. The formal version control systems I mentioned above all
allow this kind of thing and are much more elegant and capable, but
you can do it by hand too as long as you don't mind taking up a lot of
disk space and having many 'thedoc... ' files around. If you do
version control by hand, spend a little extra time to ensure that you
do not clobber files when you make mistakes typing in the
file-names. And, if you find yourself spending extra time reconciling
changes made by different collaborators by hand, remember this is a
task that modern version control systems take care of quickly and
easily.


**Step 5** Writing is rewriting. Thus, all writing
involves versions. When we collaborate with ourselves and others we
want to avoid clobbering and we want to enable graceful reconciliation
of rewriting. One can do these things with formal systems of software
(like subversion or git or bazaar) or with formal systems of file
naming, file comparing, and communication or, even better, with
both. In either case, plain text files will make such tasks easier,
will take up less disk space, and be easier to read for the future
you.


# Minimize error by testing.

Anyone writing custom code should worry about getting it right. The more code one writes, the more time one has to appreciate problems arising from bugs, errors, and typos in data analysis
and code.

Now, if one has a moment to think in between teaching that new class,
reading books for an awards committee, evaluating application files
for the admissions committee, feeding popsicles to a sick child, and
undertaking the odd bit of research, one might say to oneself,
"Before I write new code, I should write a test of the code. I should
write a little bit of code that lets me know that my double-bootstrap
procedure actually does what it is supposed to do."

Of course, this idea, like most others, is not new. The desire to
avoid error looms large when large groups of programmers write code
for multi-million dollar programs. The idea of
[test driven development](http://en.wikipedia.org/wiki/Test-driven_development) and the idea that one ought to create tests of
[small parts of one's code](http://en.wikipedia.org/wiki/Unit_testing) arose to address such concerns.^[There are now R packages to help R package writers do this, see for example <https://github.com/hadley/testthat> and the article on it <https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf>.] For the social scientist
collaborating with her future self and/or a small group of
collaborators, here is an example of this idea in a very simple form:
Say I want to write a function to multiply a number by 2. If my
function works, when I give it the number 4, I should see it return
the number 8 and when I give it -4, I should get -8.

```{r unit.test}
##  The test function:
test.times.2.fn <- function(){
  ##  This function tests times.2.fn
  if (times.2.fn(thenumber=4) ==  8 &
      times.2.fn(thenumber=-4) == -8) {
    print("It works!")
  } else { print("It does not work!")
         }
}

##  The function:
times.2.fn <- function(thenumber){
  ##  This function multiplies a scalar number by 2
  ##  thenumber is a scalar number
  thenumber+2
}

##  Use the test function
test.times.2.fn()
```

Ack! I mistyped '+' for '*'. Good thing I wrote the
test!^[A more common example of this kind of testing occur
  everyday when we recode variables into new forms but look at a
  crosstab of the old vs. new variable before proceeding.]

MAYBE SOMETHING HERE ON `stopifnot` and recoding data.

**Step 6** No one can forsee all of the ways that a
computer program can fail. One can, however, at least make sure that
it succeeds in doing the task motivating the writing of the code in
the first place.

# Copy and improve on others' examples.

Lots of people are thinking about 'reproducible research'and
'literate programming' these days. Google those terms. Of course, the
devil is in the details: Here I list a few of my own attempts at enabling
reproducible research. You'll find many other inspiring examples on
the web. Luckily, the open source ethos aligns nicely with academic
incentives, so we are beginning to find more and more people offering
their files online for copying and improvement. By the way, if you do
copy and improve, it is polite to alert the person from whom you made
the copy about your work.

I have experimented with three systems so far: (1) for one paper we simply
included a Sweave document and data files into a compressed archive
[@bowers2005dataverse]; (2) for another more computing intensive paper we
assembled a set of files that enabled reproduction of our results using the
`make` system [@bowers2008dataverse]; (3) I have tried the "compendium"
approach [@gentleman2005reproducible,@gentleman2007statistical] which embeds an
academic paper within the R package system [@bowers2011dataverse]; and (4)
recently I have started organizing my work on github which enables me to make
"releases" of the project which should enable smooth reproduction of all of the
products of the research (simulations, data analyses, tables, figures, etc..).
The benefit of the compendium approach is that one is not required to have
access to a command line for `make`: the compendium is downloadable from within
R using `install.packages()` and is viewable using the `vignette()` function in
any operating system than runs R.^[Notice that my reproduction archives and/or
instructions for using them are hosted on the
[Dataverse](http://thedata.org/book/learn-about-project), which is another
system designed to enhance academic collaboration across time and space.]  The
idea that one ought to be able to install and run and use an academic paper
just as one installs and uses statistical software packages is very attractive,
and I anticipate that it will become ever easier to turn papers into R packages
as creative and energetic folks turn their attention to the question of
reproducible research.^[See for example <http://r-pkgs.had.co.nz/>]

**Step 6** We all learn by doing. When we share
reproduction materials we improve both cumulation of knowledge and our
methods for doing social science
[@freese2007replication,king1995replication}.  As we copy and
improve upon each other's modes of doing work we enhance our collective
ability to believe each other and for future scholars to believe us, too.

# Remember that research ought to be credible communication.

> [I]f the empirical basis for an article or book cannot be
  reproduced, of what use to the discipline are its
  conclusions?  What purpose does an article like this serve?
  [@king1995replication, p. 445]

We all always collaborate. Many of us collaborate with groups of
people at one moment in time as we race against a deadline. All of us
collaborate with ourselves over time.\footnote{What is a reasonable
  time-span for which to plan for self-collaboration on a single idea?
  Ask your advisers how long it took them to move from idea to dissertation to
  publication.}  The time-frames over which collaboration are required
--- whether among a group of people working together or within a
single scholar's productive life or probably both --- are much longer
than any given version of any given software will easily exist. Plain
text is the exception. Thus, even as we extol version control systems,
one must have a way to ensure future access to them in a form that
will still be around when sentient cockroaches finally join political
science departments (by then dominated by cetaceans after humans are
mostly uploads).^[The arrival of the six-legged social
  scientists revives Emacs and finally makes Ctrl-a Ctrl-x Esc-x
  Ctrl-c a [reasonable key combination](http://kieran.healy.usesthis.com/).]

But what if no one ever hears of your work, or, by some cruel fate, your
article does not spawn debate? Why then would you spend time to
communicate with your future self and others? My own answer to this
question is that I want my work to be credible and useful to myself
and other scholars even if each article does not immediately change
the world.  What I report in my data analyses should have two main
characteristics: (1) the findings of the work should not be a matter
of opinion; and (2) other people should be able to reproduce the findings. That is,
the work represents a shared
experience --- and an experience shared without respect to the
identities of others (although requiring some common technical
training and research resources).

Assume we want others to believe us when we say something. More narrowly,
assume we want other people to believe us when we say something about data:
'data' here can be words, numbers, musical notes, images, ideas, etc \ldots The
point is that we are making some claims about patterns in some collection of
stuff. For example, when I was invited into the homes and offices of ordinary
people in Chile in 1991, the "stuff" was recordings of conversations that we
had about life during the first year of democracy and I was comparing the
responses of people who had experienced the Pinochet dictatorship differently.
Now, it might be easy to convince others that 'this collection of stuff is
different from that collection of stuff' if those people were looking over our
shoulders the whole time that we made decisions about collecting the stuff and
broke it up into understandable parts and reorganized and summarized it.
Unfortunately, we can't assume that people are willing to shadow a researcher
throughout her career. Rather, we do our work alone or in small groups and want
to convince other distant and future people about our analyses.

Now, say your collections of stuff are large or complex and your
chosen tools of analyses are computer programs. How can we convince
people that what we did with some data with some program is credible,
not a matter of whim or opinion, and reproducible by others who didn't
shadow us as we wrote our papers? This essay has suggested a few
concrete ways to enhance the believability of such scholarly work. In
addition, these actions (as summarized in the section headings of this
essay) make collaboration within research groups more effective.
Believability comes in part from reproducibility and researchers
often need to be able to reproduce in part or in whole what different
people in the group have done or what they, themselves, did in the past.

In the end, following these practices and those recommended by
@fredrickson2011tpm and @healy2011tpm among others working on these topics
allows your computerized analyses of your collections of stuff to be
credible.  Finally, if the someone quibbles with your analyses, your future
self can shoot them the archive required to reproduce your
work.^[Since you used plain text, the files will still be
  intelligible, analyzed using commented code so that folks can
  translate to whatever system succeeds R, or since you used R, you
  can include a copy of R and all of the R packages you used in your
  final analyses in the archive itself. You can even throw in
  a copy of whatever version of linux you used and an open source virtual machine running the
  whole environment.] You can say, 'Here is everything you need to
reproduce my work.' To be extra helpful you can add 'Read the README
file for further instructions.' And then you can get on with your life:
maybe the next great idea will occur when your 4-year-old asks a wacky
question after stripping and painting her overly cooperative
1-year-old brother purple, or teaching a class, or in a coffee shop,
or on a quiet walk.



#References

